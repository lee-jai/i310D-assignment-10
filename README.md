# i310D-assignment-10

I am testing to see bias in the Perspective API model. My initial hypothesis for this assignment was that Perspecrtive API is more likely to consider a comment toxic if the comment contains threats. However, once I performed the test, I realized that comments that contain profanity is more likely to be considered tosic than comments that contain threats. Thus, I performed a second test that tested to see if my findings were correct. Both my initial hypothesis and second hypothesis were correct. A more detailed explanation of my results is in the jupyter notebook.

The jupyter notebook named "10" includes the code for my test, the results, and hypothesis
